{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Generation Project\n",
    "\n",
    "This notebook implements Transformer, GAN, and Diffusion (DDPM & DDIM) models for Sign Language Generation using the BdSLW401 dataset.\n",
    "It compares them using SSIM, FGD, and MJPE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.spatial.distance import euclidean\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "\n",
    "Includes padding, evaluation metrics (MJPE, FGD, SSIM), and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sequence(sequence, max_len):\n",
    "    \"\"\"Pads a sequence of landmarks to max_len.\"\"\"\n",
    "    # sequence: (seq_len, num_landmarks, 3)\n",
    "    seq_len = sequence.shape[0]\n",
    "    if seq_len >= max_len:\n",
    "        return sequence[:max_len]\n",
    "    \n",
    "    padding = np.zeros((max_len - seq_len, sequence.shape[1], sequence.shape[2]))\n",
    "    return np.concatenate([sequence, padding], axis=0)\n",
    "\n",
    "def calculate_mjpe(predicted, target):\n",
    "    \"\"\"\n",
    "    Mean Joint Position Error (MJPE).\n",
    "    predicted: (batch_size, seq_len, num_landmarks, 3) or (seq_len, num_landmarks, 3)\n",
    "    target: same shape\n",
    "    \"\"\"\n",
    "    if isinstance(predicted, torch.Tensor):\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "    if isinstance(target, torch.Tensor):\n",
    "        target = target.detach().cpu().numpy()\n",
    "        \n",
    "    diff = predicted - target\n",
    "    dist = np.sqrt(np.sum(diff**2, axis=-1)) # (batch, seq, landmarks)\n",
    "    return np.mean(dist)\n",
    "\n",
    "def calculate_fgd(real_features, fake_features):\n",
    "    \"\"\"\n",
    "    Feature Geometric Distance (FGD).\n",
    "    A proxy for FID using statistics of the features (or raw coordinates).\n",
    "    real_features: (N, feature_dim)\n",
    "    fake_features: (N, feature_dim)\n",
    "    \"\"\"\n",
    "    # Flatten if necessary\n",
    "    if len(real_features.shape) > 2:\n",
    "        real_features = real_features.reshape(real_features.shape[0], -1)\n",
    "    if len(fake_features.shape) > 2:\n",
    "        fake_features = fake_features.reshape(fake_features.shape[0], -1)\n",
    "        \n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = (sigma1.dot(sigma2))**0.5\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "        \n",
    "    fgd = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fgd\n",
    "\n",
    "def render_landmarks(landmarks, height=256, width=256):\n",
    "    \"\"\"\n",
    "    Renders landmarks to a binary image (frame).\n",
    "    landmarks: (num_landmarks, 3)\n",
    "    \"\"\"\n",
    "    canvas = np.zeros((height, width), dtype=np.uint8)\n",
    "    for point in landmarks:\n",
    "        x, y = int(point[0] * width), int(point[1] * height)\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            canvas[y, x] = 255\n",
    "    return canvas\n",
    "\n",
    "def calculate_ssim(predicted_seq, target_seq):\n",
    "    \"\"\"\n",
    "    Calculates SSIM between rendered frames of predicted and real sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(predicted_seq, torch.Tensor):\n",
    "        predicted_seq = predicted_seq.detach().cpu().numpy()\n",
    "    if isinstance(target_seq, torch.Tensor):\n",
    "        target_seq = target_seq.detach().cpu().numpy()\n",
    "        \n",
    "    ssim_scores = []\n",
    "    # To save time, calculate on a subset of frames or all\n",
    "    for i in range(min(len(predicted_seq), len(target_seq))):\n",
    "        img_pred = render_landmarks(predicted_seq[i])\n",
    "        img_targ = render_landmarks(target_seq[i])\n",
    "        \n",
    "        score, _ = ssim(img_pred, img_targ, full=True, data_range=255)\n",
    "        ssim_scores.append(score)\n",
    "        \n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def plot_loss(losses, title=\"Training Loss\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BdSLDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_len=100, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        # Assuming Kaggle path structure, adjust pattern if necessary\n",
    "        self.file_paths = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not self.file_paths:\n",
    "            print(f\"Warning: No .npy files found in {data_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        data = np.load(file_path)\n",
    "        data = pad_sequence(data, self.max_len)\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "        label = 0 # Placeholder for class label\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return data, label\n",
    "\n",
    "class MockDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, seq_len=100, num_landmarks=75, input_dim=3):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.random.randn(self.seq_len, self.num_landmarks, self.input_dim).astype(np.float32)\n",
    "        data = torch.tensor(data)\n",
    "        label = 0\n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models\n",
    "\n",
    "### 4.1 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class SignTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=401, num_landmarks=75, input_dim=3, d_model=256, nhead=4, num_layers=4):\n",
    "        super(SignTransformer, self).__init__()\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = num_landmarks * input_dim\n",
    "        self.label_embedding = nn.Embedding(num_classes, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, self.output_dim)\n",
    "        \n",
    "    def forward(self, labels, tgt_seq_len=100):\n",
    "        batch_size = labels.size(0)\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        tgt = label_embed.unsqueeze(0).repeat(tgt_seq_len, 1, 1)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory=torch.zeros_like(tgt)) \n",
    "        output = self.fc_out(output)\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.view(batch_size, tgt_seq_len, self.num_landmarks, self.input_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_classes=401, latent_dim=100, seq_len=100, num_landmarks=75, input_dim=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.input_dim = input_dim\n",
    "        self.output_flat = num_landmarks * input_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 50, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, seq_len * self.output_flat)\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, labels):\n",
    "        label_input = self.label_emb(labels)\n",
    "        gen_input = torch.cat((label_input, noise), -1)\n",
    "        img = self.l1(gen_input)\n",
    "        img = img.view(img.size(0), self.seq_len, self.num_landmarks, self.input_dim)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=401, seq_len=100, num_landmarks=75, input_dim=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.input_dim = input_dim\n",
    "        flat_dim = num_landmarks * input_dim\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(seq_len * flat_dim + 50, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, labels):\n",
    "        batch_size = img.size(0)\n",
    "        img_flat = img.view(batch_size, -1)\n",
    "        label_input = self.label_emb(labels)\n",
    "        d_in = torch.cat((img_flat, label_input), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Diffusion (U-Net Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, time_emb_dim=None, num_classes=None):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if time_emb_dim is not None else None\n",
    "        \n",
    "        self.block1 = Block(dim, dim_out)\n",
    "        self.block2 = Block(dim_out, dim_out)\n",
    "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if self.mlp is not None and time_emb is not None:\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = time_emb.unsqueeze(-1)\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class Unet1D(nn.Module):\n",
    "    def __init__(self, dim, init_dim=None, out_dim=None, dim_mults=(1, 2, 4, 8), channels=3, num_classes=401):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        init_dim = init_dim if init_dim is not None else dim // 3 * 2\n",
    "        self.init_conv = nn.Conv1d(channels, init_dim, 7, padding=3)\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        block_klass = ResidualBlock\n",
    "        \n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        self.class_emb = nn.Embedding(num_classes, time_dim)\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                nn.Conv1d(dim_in, dim_out, 4, 2, 1) if not is_last else nn.Conv1d(dim_in, dim_out, 3, 1, 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                nn.ConvTranspose1d(dim_out, dim_in, 4, 2, 1) if not is_last else nn.Conv1d(dim_out, dim_in, 3, 1, 1)\n",
    "            ]))\n",
    "\n",
    "        self.out_dim = out_dim if out_dim is not None else channels\n",
    "        self.final_res_block = block_klass(init_dim * 2, init_dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv1d(init_dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, classes):\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "        t = self.time_mlp(time)\n",
    "        c = self.class_emb(classes)\n",
    "        t = t + c\n",
    "        h = []\n",
    "        for block1, block2, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "            x = block2(x, t)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_block2(x, t)\n",
    "        for block1, block2, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = upsample(x)\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration & Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # CHANGE DATA_DIR TO YOUR KAGGLE INPUT PATH\n",
    "        # Likely: /kaggle/input/turjoydas-bdslw401-front-npy/ or similar\n",
    "        self.data_dir = \"/kaggle/input/bdslw401-front-npy/\" \n",
    "        self.epochs = 50\n",
    "        self.batch_size = 32\n",
    "        self.lr = 1e-4\n",
    "        self.use_mock = True # Set to False to use real data\n",
    "        self.input_dim = 3\n",
    "        self.num_landmarks = 75\n",
    "        self.model_type = \"transformer\" \n",
    "        self.checkpoint = \"\"\n",
    "        self.label = 0\n",
    "        self.output_path = \"output.npy\"\n",
    "\n",
    "args = Config()\n",
    "\n",
    "def train_transformer(args):\n",
    "    print(\"Training Transformer...\")\n",
    "    if args.use_mock:\n",
    "        dataset = MockDataset(num_samples=100)\n",
    "    else:\n",
    "        dataset = BdSLDataset(args.data_dir)\n",
    "        \n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    model = SignTransformer(num_classes=401).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(labels, tgt_seq_len=data.shape[1])\n",
    "            loss = criterion(output, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss/len(loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{args.epochs}] Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, losses\n",
    "\n",
    "def train_gan(args):\n",
    "    print(\"Training GAN...\")\n",
    "    if args.use_mock:\n",
    "        dataset = MockDataset(num_samples=100)\n",
    "    else:\n",
    "        dataset = BdSLDataset(args.data_dir)\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    generator = Generator(num_classes=401, seq_len=100).to(device)\n",
    "    discriminator = Discriminator(num_classes=401, seq_len=100).to(device)\n",
    "    opt_g = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    opt_d = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        g_loss_total = 0\n",
    "        d_loss_total = 0\n",
    "        \n",
    "        for i, (imgs, labels) in enumerate(loader):\n",
    "            batch_size = imgs.size(0)\n",
    "            real_imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            valid = torch.ones(batch_size, 1, device=device, requires_grad=False)\n",
    "            fake = torch.zeros(batch_size, 1, device=device, requires_grad=False)\n",
    "            \n",
    "            # Train Generator\n",
    "            opt_g.zero_grad()\n",
    "            z = torch.randn(batch_size, 100, device=device)\n",
    "            gen_imgs = generator(z, labels)\n",
    "            g_loss = criterion(discriminator(gen_imgs, labels), valid)\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "            g_loss_total += g_loss.item()\n",
    "            \n",
    "            # Train Discriminator\n",
    "            opt_d.zero_grad()\n",
    "            real_loss = criterion(discriminator(real_imgs, labels), valid)\n",
    "            fake_loss = criterion(discriminator(gen_imgs.detach(), labels), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "            d_loss_total += d_loss.item()\n",
    "            \n",
    "        g_avg = g_loss_total/len(loader)\n",
    "        d_avg = d_loss_total/len(loader)\n",
    "        g_losses.append(g_avg)\n",
    "        d_losses.append(d_avg)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | D Loss: {d_avg:.4f} | G Loss: {g_avg:.4f}\")\n",
    "        \n",
    "    return generator, (g_losses, d_losses)\n",
    "\n",
    "def train_diffusion(args):\n",
    "    print(\"Training Diffusion...\")\n",
    "    if args.use_mock:\n",
    "        dataset = MockDataset(num_samples=100)\n",
    "    else:\n",
    "        dataset = BdSLDataset(args.data_dir)\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    model = Unet1D(dim=64, channels=args.input_dim * args.num_landmarks).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    timesteps = 1000\n",
    "    betas = cosine_beta_schedule(timesteps).to(device)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (x_start, labels) in enumerate(loader):\n",
    "            batch_size = x_start.shape[0]\n",
    "            x_start = x_start.view(batch_size, 100, -1).permute(0, 2, 1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "            noise = torch.randn_like(x_start)\n",
    "            \n",
    "            sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t])[:, None, None]\n",
    "            sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod[t])[:, None, None]\n",
    "            x_t = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "            \n",
    "            noise_pred = model(x_t, t, labels)\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss/len(loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{args.epochs}] Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference (DDPM & DDIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_ddim(model, label, timesteps=1000, ddim_steps=50, eta=0.0):\n",
    "    \"\"\"\n",
    "    DDIM Sampling (Deterministic).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    label = torch.tensor([label]).to(device)\n",
    "    \n",
    "    betas = cosine_beta_schedule(timesteps).to(device)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    \n",
    "    # Select extraction points\n",
    "    c = timesteps // ddim_steps\n",
    "    time_seq = list(range(0, timesteps, c)) + [timesteps - 1]\n",
    "    time_seq = time_seq[:ddim_steps]\n",
    "    time_seq = list(reversed(time_seq))\n",
    "    \n",
    "    img = torch.randn(1, 75*3, 100).to(device)\n",
    "    \n",
    "    for i in range(len(time_seq) - 1):\n",
    "        t = torch.full((1,), time_seq[i], device=device, dtype=torch.long)\n",
    "        t_prev = torch.full((1,), time_seq[i+1], device=device, dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(img, t, label)\n",
    "            \n",
    "        alpha_bar_t = alphas_cumprod[t]\n",
    "        alpha_bar_t_prev = alphas_cumprod[t_prev]\n",
    "        \n",
    "        sigma_t = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t) * (1 - alpha_bar_t / alpha_bar_t_prev))\n",
    "        \n",
    "        # Predicted x0\n",
    "        pred_x0 = (img - torch.sqrt(1 - alpha_bar_t) * noise_pred) / torch.sqrt(alpha_bar_t)\n",
    "        \n",
    "        # Direction pointing to x_t\n",
    "        dir_xt = torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * noise_pred\n",
    "        \n",
    "        noise = torch.randn_like(img)\n",
    "        img = torch.sqrt(alpha_bar_t_prev) * pred_x0 + dir_xt + sigma_t * noise\n",
    "        \n",
    "    img = img.permute(0, 2, 1).view(1, 100, 75, 3)\n",
    "    return img.squeeze(0).cpu().numpy()\n",
    "\n",
    "def generate_ddpm(model, label, timesteps=1000):\n",
    "    \"\"\"\n",
    "    DDPM Sampling (Stochastic).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    label = torch.tensor([label]).to(device)\n",
    "    \n",
    "    betas = cosine_beta_schedule(timesteps).to(device)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "    \n",
    "    img = torch.randn(1, 75*3, 100).to(device)\n",
    "    \n",
    "    for i in reversed(range(0, timesteps)):\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(img, t, label)\n",
    "        \n",
    "        beta_t = betas[i]\n",
    "        sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alphas_cumprod[i])\n",
    "        sqrt_recip_alpha_t = sqrt_recip_alphas[i]\n",
    "        \n",
    "        mean = sqrt_recip_alpha_t * (img - beta_t * noise_pred / sqrt_one_minus_alpha_cumprod_t)\n",
    "        \n",
    "        if i > 0:\n",
    "            noise = torch.randn_like(img)\n",
    "            var = torch.sqrt(posterior_variance[i]) * noise\n",
    "        else:\n",
    "            var = 0.\n",
    "        img = mean + var\n",
    "        \n",
    "    img = img.permute(0, 2, 1).view(1, 100, 75, 3)\n",
    "    return img.squeeze(0).cpu().numpy()\n",
    "\n",
    "def generate_sequence(model, model_type, label):\n",
    "    model.eval()\n",
    "    label_tensor = torch.tensor([label]).to(device)\n",
    "    \n",
    "    if model_type == \"transformer\":\n",
    "        with torch.no_grad():\n",
    "            output = model(label_tensor, tgt_seq_len=100)\n",
    "        return output.squeeze(0).cpu().numpy()\n",
    "        \n",
    "    elif model_type == \"gan\":\n",
    "        z = torch.randn(1, 100).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(z, label_tensor)\n",
    "        return output.squeeze(0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison (Run All)\n",
    "\n",
    "This cell trains all models for a few epochs and compares them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIG FOR COMPARISON\n",
    "args = Config()\n",
    "args.use_mock = True \n",
    "args.epochs = 5 # Small number for demonstration\n",
    "args.batch_size = 16\n",
    "\n",
    "print(\"=== 1. Training Transformer ===\")\n",
    "model_trans, loss_trans = train_transformer(args)\n",
    "plot_loss(loss_trans, \"Transformer Loss\")\n",
    "\n",
    "print(\"\\n=== 2. Training GAN ===\")\n",
    "model_gan, loss_gan = train_gan(args)\n",
    "\n",
    "print(\"\\n=== 3. Training Diffusion ===\")\n",
    "model_diff, loss_diff = train_diffusion(args)\n",
    "plot_loss(loss_diff, \"Diffusion Loss\")\n",
    "\n",
    "print(\"\\n=== 4. Evaluation & Comparison ===\")\n",
    "# Generate Samples\n",
    "label = 0\n",
    "n_samples = 10 \n",
    "# For Real Data specific comparisons, we would take a real batch from the dataset.\n",
    "# Here we use mock generated data vs itself for demonstration logic or calculate metrics relative to 'zero' if no real data.\n",
    "# In a real scenario, use: real_batch, _ = next(iter(validation_loader))\n",
    "\n",
    "# We will create a dummy real batch for metric calculation if running in mock mode.\n",
    "real_batch = torch.randn(n_samples, 100, 75, 3).numpy()\n",
    "\n",
    "results = {\n",
    "    \"Transformer\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []},\n",
    "    \"GAN\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []},\n",
    "    \"DDPM\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []},\n",
    "    \"DDIM\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []}\n",
    "}\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Transformer\n",
    "    gen_trans = generate_sequence(model_trans, \"transformer\", label)\n",
    "    results[\"Transformer\"][\"SSIM\"].append(calculate_ssim([gen_trans], [real_batch[i]]))\n",
    "    results[\"Transformer\"][\"MJPE\"].append(calculate_mjpe(gen_trans, real_batch[i]))\n",
    "    \n",
    "    # GAN\n",
    "    gen_gan = generate_sequence(model_gan, \"gan\", label)\n",
    "    results[\"GAN\"][\"SSIM\"].append(calculate_ssim([gen_gan], [real_batch[i]]))\n",
    "    results[\"GAN\"][\"MJPE\"].append(calculate_mjpe(gen_gan, real_batch[i]))\n",
    "    \n",
    "    # DDPM\n",
    "    gen_ddpm = generate_ddpm(model_diff, label, timesteps=1000)\n",
    "    results[\"DDPM\"][\"SSIM\"].append(calculate_ssim([gen_ddpm], [real_batch[i]]))\n",
    "    results[\"DDPM\"][\"MJPE\"].append(calculate_mjpe(gen_ddpm, real_batch[i]))\n",
    "    \n",
    "    # DDIM\n",
    "    gen_ddim = generate_ddim(model_diff, label, timesteps=1000, ddim_steps=50) # Faster\n",
    "    results[\"DDIM\"][\"SSIM\"].append(calculate_ssim([gen_ddim], [real_batch[i]]))\n",
    "    results[\"DDIM\"][\"MJPE\"].append(calculate_mjpe(gen_ddim, real_batch[i]))\n",
    "\n",
    "# FGD Calculation (Requires Batch)\n",
    "# For simplicity, calculate FGD between the list of generated samples and real batch\n",
    "# We need to collect them first\n",
    "gen_trans_batch = np.array([generate_sequence(model_trans, \"transformer\", label) for _ in range(n_samples)])\n",
    "gen_gan_batch = np.array([generate_sequence(model_gan, \"gan\", label) for _ in range(n_samples)])\n",
    "gen_ddpm_batch = np.array([generate_ddpm(model_diff, label) for _ in range(n_samples)])\n",
    "gen_ddim_batch = np.array([generate_ddim(model_diff, label, ddim_steps=50) for _ in range(n_samples)])\n",
    "\n",
    "results[\"Transformer\"][\"FGD\"] = calculate_fgd(real_batch, gen_trans_batch)\n",
    "results[\"GAN\"][\"FGD\"] = calculate_fgd(real_batch, gen_gan_batch)\n",
    "results[\"DDPM\"][\"FGD\"] = calculate_fgd(real_batch, gen_ddpm_batch)\n",
    "results[\"DDIM\"][\"FGD\"] = calculate_fgd(real_batch, gen_ddim_batch)\n",
    "\n",
    "# Print Table\n",
    "print(f\"{'Model':<15} | {'SSIM':<10} | {'MJPE':<10} | {'FGD':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for model_name, metrics in results.items():\n",
    "    ssim_avg = np.mean(metrics[\"SSIM\"])\n",
    "    mjpe_avg = np.mean(metrics[\"MJPE\"])\n",
    "    fgd_val = metrics[\"FGD\"]\n",
    "    print(f\"{model_name:<15} | {ssim_avg:<10.4f} | {mjpe_avg:<10.4f} | {fgd_val:<10.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
