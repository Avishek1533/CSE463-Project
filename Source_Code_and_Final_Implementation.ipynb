{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 14380228,
          "sourceType": "datasetVersion",
          "datasetId": 9183606
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "1pJAYce1ZIYi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:31:55.263763Z",
          "iopub.execute_input": "2026-01-03T10:31:55.264096Z",
          "iopub.status.idle": "2026-01-03T10:31:56.732753Z",
          "shell.execute_reply.started": "2026-01-03T10:31:55.264060Z",
          "shell.execute_reply": "2026-01-03T10:31:56.732004Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "4e241ca5-784b-407b-c9d7-ef91f040313d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.31-py3-none-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting absl-py~=2.3 (from mediapipe)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.0.2)\n",
            "Collecting sounddevice~=0.5 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: flatbuffers~=25.9 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.12.19)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice~=0.5->mediapipe) (2.23)\n",
            "Downloading mediapipe-0.10.31-py3-none-manylinux_2_28_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: absl-py, sounddevice, mediapipe\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "Successfully installed absl-py-2.3.1 mediapipe-0.10.31 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl"
                ]
              },
              "id": "77ea6931a92140afa357b87f9ba33a54"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Setup"
      ],
      "metadata": {
        "id": "NVAMsBLhM1Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from scipy.spatial.distance import euclidean\n",
        "import random\n",
        "\n",
        "# Constants\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v53nYFn2ZOOY",
        "outputId": "7ba4d295-e886-4642-d6e4-621c0a4bd404",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:32:40.885699Z",
          "iopub.execute_input": "2026-01-03T10:32:40.886075Z",
          "iopub.status.idle": "2026-01-03T10:33:11.148690Z",
          "shell.execute_reply.started": "2026-01-03T10:32:40.886042Z",
          "shell.execute_reply": "2026-01-03T10:33:11.147903Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "Yy6HQhRhM_-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pad_sequence(sequence, max_len):\n",
        "    \"\"\"Pads a sequence of landmarks to max_len.\"\"\"\n",
        "    # sequence: (seq_len, num_landmarks, 3)\n",
        "    seq_len = sequence.shape[0]\n",
        "    if seq_len >= max_len:\n",
        "        return sequence[:max_len]\n",
        "\n",
        "    padding = np.zeros((max_len - seq_len, sequence.shape[1], sequence.shape[2]))\n",
        "    return np.concatenate([sequence, padding], axis=0)\n",
        "\n",
        "def calculate_mjpe(predicted, target):\n",
        "    \"\"\"\n",
        "    Mean Joint Position Error (MJPE).\n",
        "    predicted: (batch_size, seq_len, num_landmarks, 3) or (seq_len, num_landmarks, 3)\n",
        "    target: same shape\n",
        "    \"\"\"\n",
        "    if isinstance(predicted, torch.Tensor):\n",
        "        predicted = predicted.detach().cpu().numpy()\n",
        "    if isinstance(target, torch.Tensor):\n",
        "        target = target.detach().cpu().numpy()\n",
        "\n",
        "    diff = predicted - target\n",
        "    dist = np.sqrt(np.sum(diff**2, axis=-1)) # (batch, seq, landmarks)\n",
        "    return np.mean(dist)\n",
        "\n",
        "def calculate_fgd(real_features, fake_features):\n",
        "    \"\"\"\n",
        "    Feature Geometric Distance (FGD).\n",
        "    A proxy for FID using statistics of the features (or raw coordinates).\n",
        "    real_features: (N, feature_dim)\n",
        "    fake_features: (N, feature_dim)\n",
        "    \"\"\"\n",
        "    # Flatten if necessary\n",
        "    if len(real_features.shape) > 2:\n",
        "        real_features = real_features.reshape(real_features.shape[0], -1)\n",
        "    if len(fake_features.shape) > 2:\n",
        "        fake_features = fake_features.reshape(fake_features.shape[0], -1)\n",
        "\n",
        "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "    covmean = (sigma1.dot(sigma2))**0.5\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fgd = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fgd\n",
        "\n",
        "def render_landmarks(landmarks, height=256, width=256):\n",
        "    \"\"\"\n",
        "    Renders landmarks to a binary image (frame).\n",
        "    landmarks: (num_landmarks, 3)\n",
        "    \"\"\"\n",
        "    canvas = np.zeros((height, width), dtype=np.uint8)\n",
        "    for point in landmarks:\n",
        "        x, y = int(point[0] * width), int(point[1] * height)\n",
        "        if 0 <= x < width and 0 <= y < height:\n",
        "            canvas[y, x] = 255\n",
        "    return canvas\n",
        "\n",
        "def calculate_ssim(predicted_seq, target_seq):\n",
        "    \"\"\"\n",
        "    Calculates SSIM between rendered frames of predicted and real sequences.\n",
        "    \"\"\"\n",
        "    if isinstance(predicted_seq, torch.Tensor):\n",
        "        predicted_seq = predicted_seq.detach().cpu().numpy()\n",
        "    if isinstance(target_seq, torch.Tensor):\n",
        "        target_seq = target_seq.detach().cpu().numpy()\n",
        "\n",
        "    ssim_scores = []\n",
        "    # To save time, calculate on a subset of frames or all\n",
        "    for i in range(min(len(predicted_seq), len(target_seq))):\n",
        "        img_pred = render_landmarks(predicted_seq[i])\n",
        "        img_targ = render_landmarks(target_seq[i])\n",
        "\n",
        "        score, _ = ssim(img_pred, img_targ, full=True, data_range=255)\n",
        "        ssim_scores.append(score)\n",
        "\n",
        "    return np.mean(ssim_scores)\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    Cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def plot_loss(losses, title=\"Training Loss\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses, label=\"Loss\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uhP4lsPJNC84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Implementation"
      ],
      "metadata": {
        "id": "lg7TFIe2NfJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BdSLDataset(Dataset):\n",
        "    def __init__(self, data_dir, max_len=100, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        # Assuming Kaggle path structure, adjust pattern if necessary\n",
        "        self.file_paths = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "        if not self.file_paths:\n",
        "            print(f\"Warning: No .npy files found in {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        data = np.load(file_path)\n",
        "        data = pad_sequence(data, self.max_len)\n",
        "        data = torch.tensor(data, dtype=torch.float32)\n",
        "        label = 0 # Placeholder for class label\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data, label\n",
        "\n",
        "class MockDataset(Dataset):\n",
        "    def __init__(self, num_samples=100, seq_len=100, num_landmarks=75, input_dim=3):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.num_landmarks = num_landmarks\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.random.randn(self.seq_len, self.num_landmarks, self.input_dim).astype(np.float32)\n",
        "        data = torch.tensor(data)\n",
        "        label = 0\n",
        "        return data, label\n"
      ],
      "metadata": {
        "id": "db2b3f29",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T10:34:09.742112Z",
          "iopub.execute_input": "2026-01-03T10:34:09.742817Z",
          "iopub.status.idle": "2026-01-03T10:34:09.751273Z",
          "shell.execute_reply.started": "2026-01-03T10:34:09.742790Z",
          "shell.execute_reply": "2026-01-03T10:34:09.750476Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Transformer"
      ],
      "metadata": {
        "id": "1PahdF3gNsrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class SignTransformer(nn.Module):\n",
        "    def __init__(self, num_classes=401, num_landmarks=75, input_dim=3, d_model=256, nhead=4, num_layers=4):\n",
        "        super(SignTransformer, self).__init__()\n",
        "        self.num_landmarks = num_landmarks\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = num_landmarks * input_dim\n",
        "        self.label_embedding = nn.Embedding(num_classes, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, self.output_dim)\n",
        "\n",
        "    def forward(self, labels, tgt_seq_len=100):\n",
        "        batch_size = labels.size(0)\n",
        "        label_embed = self.label_embedding(labels)\n",
        "        tgt = label_embed.unsqueeze(0).repeat(tgt_seq_len, 1, 1)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        output = self.transformer_decoder(tgt, memory=torch.zeros_like(tgt))\n",
        "        output = self.fc_out(output)\n",
        "        output = output.transpose(0, 1)\n",
        "        output = output.view(batch_size, tgt_seq_len, self.num_landmarks, self.input_dim)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "VQewwX5PN9K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GAN"
      ],
      "metadata": {
        "id": "FnBKRCyXDR99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_classes=401, latent_dim=100, seq_len=100, num_landmarks=75, input_dim=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_landmarks = num_landmarks\n",
        "        self.input_dim = input_dim\n",
        "        self.output_flat = num_landmarks * input_dim\n",
        "        self.label_emb = nn.Embedding(num_classes, 50)\n",
        "\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Linear(latent_dim + 50, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, seq_len * self.output_flat)\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_input = self.label_emb(labels)\n",
        "        gen_input = torch.cat((label_input, noise), -1)\n",
        "        img = self.l1(gen_input)\n",
        "        img = img.view(img.size(0), self.seq_len, self.num_landmarks, self.input_dim)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_classes=401, seq_len=100, num_landmarks=75, input_dim=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.num_landmarks = num_landmarks\n",
        "        self.input_dim = input_dim\n",
        "        flat_dim = num_landmarks * input_dim\n",
        "        self.label_emb = nn.Embedding(num_classes, 50)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(seq_len * flat_dim + 50, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        batch_size = img.size(0)\n",
        "        img_flat = img.view(batch_size, -1)\n",
        "        label_input = self.label_emb(labels)\n",
        "        d_in = torch.cat((img_flat, label_input), -1)\n",
        "        validity = self.model(d_in)\n",
        "        return validity\n"
      ],
      "metadata": {
        "id": "-Yx1i6NKlCq4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Diffusion (U-Net Backbone)"
      ],
      "metadata": {
        "id": "7gHsINLMDR9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv1d(dim, dim_out, 3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift=None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "        if scale_shift is not None:\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, time_emb_dim=None, num_classes=None):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, dim_out * 2)\n",
        "        ) if time_emb_dim is not None else None\n",
        "\n",
        "        self.block1 = Block(dim, dim_out)\n",
        "        self.block2 = Block(dim_out, dim_out)\n",
        "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        scale_shift = None\n",
        "        if self.mlp is not None and time_emb is not None:\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            time_emb = time_emb.unsqueeze(-1)\n",
        "            scale_shift = time_emb.chunk(2, dim=1)\n",
        "        h = self.block1(x, scale_shift=scale_shift)\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class Unet1D(nn.Module):\n",
        "    def __init__(self, dim, init_dim=None, out_dim=None, dim_mults=(1, 2, 4, 8), channels=3, num_classes=401):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.num_classes = num_classes\n",
        "        init_dim = init_dim if init_dim is not None else dim // 3 * 2\n",
        "        self.init_conv = nn.Conv1d(channels, init_dim, 7, padding=3)\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        block_klass = ResidualBlock\n",
        "\n",
        "        time_dim = dim * 4\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPosEmb(dim),\n",
        "            nn.Linear(dim, time_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        self.class_emb = nn.Embedding(num_classes, time_dim)\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                nn.Conv1d(dim_in, dim_out, 4, 2, 1) if not is_last else nn.Conv1d(dim_in, dim_out, 3, 1, 1)\n",
        "            ]))\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (len(in_out) - 1)\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                nn.ConvTranspose1d(dim_out, dim_in, 4, 2, 1) if not is_last else nn.Conv1d(dim_out, dim_in, 3, 1, 1)\n",
        "            ]))\n",
        "\n",
        "        self.out_dim = out_dim if out_dim is not None else channels\n",
        "        self.final_res_block = block_klass(init_dim * 2, init_dim, time_emb_dim=time_dim)\n",
        "        self.final_conv = nn.Conv1d(init_dim, self.out_dim, 1)\n",
        "\n",
        "    def forward(self, x, time, classes):\n",
        "        x = self.init_conv(x)\n",
        "        r = x.clone()\n",
        "        t = self.time_mlp(time)\n",
        "        c = self.class_emb(classes)\n",
        "        t = t + c\n",
        "        h = []\n",
        "        for block1, block2, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            h.append(x)\n",
        "            x = block2(x, t)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_block2(x, t)\n",
        "        for block1, block2, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block2(x, t)\n",
        "            x = upsample(x)\n",
        "        x = torch.cat((x, r), dim=1)\n",
        "        x = self.final_res_block(x, t)\n",
        "        return self.final_conv(x)\n"
      ],
      "metadata": {
        "id": "2YFOIKnjORVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "ysqcfsmcOc0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # CHANGE DATA_DIR TO YOUR KAGGLE INPUT PATH\n",
        "        # Likely: /kaggle/input/turjoydas-bdslw401-front-npy/ or similar\n",
        "        self.data_dir = \"/kaggle/input/bdslw401-front-npy/\"\n",
        "        self.epochs = 50\n",
        "        self.batch_size = 32\n",
        "        self.lr = 1e-4\n",
        "        self.use_mock = True # Set to False to use real data\n",
        "        self.input_dim = 3\n",
        "        self.num_landmarks = 75\n",
        "        self.model_type = \"transformer\"\n",
        "        self.checkpoint = \"\"\n",
        "        self.label = 0\n",
        "        self.output_path = \"output.npy\"\n",
        "\n",
        "args = Config()\n"
      ],
      "metadata": {
        "id": "HzFtI0AYOaoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Transformer"
      ],
      "metadata": {
        "id": "es-tD_cfOv-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a80cf1a1",
        "outputId": "2d34625f-0a85-4be7-c9b1-c52f129fe74a"
      },
      "source": [
        "def train_transformer(args):\n",
        "    print(\"Training Transformer...\")\n",
        "    if args.use_mock:\n",
        "        dataset = MockDataset(num_samples=100)\n",
        "    else:\n",
        "        dataset = BdSLDataset(args.data_dir)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    model = SignTransformer(num_classes=401).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = model(labels, tgt_seq_len=data.shape[1])\n",
        "            loss = criterion(output, data)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss/len(loader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{args.epochs}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model, losses\n",
        "\n",
        "print(\"Training Transformer...\")\n",
        "args = Config()\n",
        "args.use_mock = True\n",
        "args.epochs = 50 # Small number for demonstration\n",
        "args.batch_size = 16\n",
        "print(\"Best Transformer model Train and Save\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Transformer Model...\n",
            "Epoch 1/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.4348 - loss: 1.6599 - val_accuracy: 0.6429 - val_loss: 1.1095\n",
            "Epoch 2/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6041 - loss: 1.2655 - val_accuracy: 0.6494 - val_loss: 1.0859\n",
            "Epoch 3/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6204 - loss: 1.2220 - val_accuracy: 0.6558 - val_loss: 1.0722\n",
            "Epoch 4/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6236 - loss: 1.2125 - val_accuracy: 0.6526 - val_loss: 1.0724\n",
            "Epoch 5/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6292 - loss: 1.1994 - val_accuracy: 0.6585 - val_loss: 1.0609\n",
            "Epoch 6/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6335 - loss: 1.1862 - val_accuracy: 0.6627 - val_loss: 1.0466\n",
            "Epoch 7/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6369 - loss: 1.1760 - val_accuracy: 0.6637 - val_loss: 1.0453\n",
            "Epoch 8/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6378 - loss: 1.1736 - val_accuracy: 0.6650 - val_loss: 1.0377\n",
            "Epoch 9/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6385 - loss: 1.1704 - val_accuracy: 0.6638 - val_loss: 1.0380\n",
            "Epoch 10/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6410 - loss: 1.1621 - val_accuracy: 0.6628 - val_loss: 1.0426\n",
            "Epoch 11/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6397 - loss: 1.1609 - val_accuracy: 0.6673 - val_loss: 1.0366\n",
            "Epoch 12/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6432 - loss: 1.1594 - val_accuracy: 0.6619 - val_loss: 1.0436\n",
            "Epoch 13/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6425 - loss: 1.1526 - val_accuracy: 0.6611 - val_loss: 1.0411\n",
            "Epoch 14/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6454 - loss: 1.1510 - val_accuracy: 0.6660 - val_loss: 1.0294\n",
            "Epoch 15/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6456 - loss: 1.1473 - val_accuracy: 0.6657 - val_loss: 1.0374\n",
            "Epoch 16/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6463 - loss: 1.1450 - val_accuracy: 0.6676 - val_loss: 1.0303\n",
            "Epoch 17/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6477 - loss: 1.1466 - val_accuracy: 0.6697 - val_loss: 1.0270\n",
            "Epoch 18/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6483 - loss: 1.1411 - val_accuracy: 0.6696 - val_loss: 1.0267\n",
            "Epoch 19/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6502 - loss: 1.1402 - val_accuracy: 0.6689 - val_loss: 1.0249\n",
            "Epoch 20/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6500 - loss: 1.1397 - val_accuracy: 0.6702 - val_loss: 1.0254\n",
            "Epoch 21/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6505 - loss: 1.1359 - val_accuracy: 0.6739 - val_loss: 1.0228\n",
            "Epoch 22/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6488 - loss: 1.1432 - val_accuracy: 0.6678 - val_loss: 1.0291\n",
            "Epoch 23/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6516 - loss: 1.1318 - val_accuracy: 0.6683 - val_loss: 1.0263\n",
            "Epoch 24/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6506 - loss: 1.1358 - val_accuracy: 0.6691 - val_loss: 1.0235\n",
            "Epoch 25/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6513 - loss: 1.1342 - val_accuracy: 0.6713 - val_loss: 1.0243\n",
            "Epoch 26/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6506 - loss: 1.1261 - val_accuracy: 0.6715 - val_loss: 1.0214\n",
            "Epoch 27/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6535 - loss: 1.1176 - val_accuracy: 0.6701 - val_loss: 1.0253\n",
            "Epoch 28/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6529 - loss: 1.1304 - val_accuracy: 0.6684 - val_loss: 1.0215\n",
            "Epoch 29/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6551 - loss: 1.1167 - val_accuracy: 0.6729 - val_loss: 1.0209\n",
            "Epoch 30/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6528 - loss: 1.1208 - val_accuracy: 0.6686 - val_loss: 1.0319\n",
            "Epoch 31/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6537 - loss: 1.1187 - val_accuracy: 0.6690 - val_loss: 1.0245\n",
            "Epoch 32/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6527 - loss: 1.1152 - val_accuracy: 0.6678 - val_loss: 1.0253\n",
            "Epoch 33/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6512 - loss: 1.1187 - val_accuracy: 0.6733 - val_loss: 1.0170\n",
            "Epoch 34/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6569 - loss: 1.1111 - val_accuracy: 0.6735 - val_loss: 1.0168\n",
            "Epoch 35/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6552 - loss: 1.1173 - val_accuracy: 0.6762 - val_loss: 1.0197\n",
            "Epoch 36/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6558 - loss: 1.1107 - val_accuracy: 0.6727 - val_loss: 1.0212\n",
            "Epoch 37/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6586 - loss: 1.1092 - val_accuracy: 0.6720 - val_loss: 1.0199\n",
            "Epoch 38/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6547 - loss: 1.1144 - val_accuracy: 0.6756 - val_loss: 1.0162\n",
            "Epoch 39/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6559 - loss: 1.1100 - val_accuracy: 0.6714 - val_loss: 1.0190\n",
            "Epoch 40/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6582 - loss: 1.1091 - val_accuracy: 0.6736 - val_loss: 1.0171\n",
            "Epoch 41/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6565 - loss: 1.1070 - val_accuracy: 0.6655 - val_loss: 1.0307\n",
            "Epoch 42/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6553 - loss: 1.1057 - val_accuracy: 0.6719 - val_loss: 1.0234\n",
            "Epoch 43/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6578 - loss: 1.1039 - val_accuracy: 0.6713 - val_loss: 1.0201\n",
            "Best Transformer model trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GAN"
      ],
      "metadata": {
        "id": "OBVgwGX5O3k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(args):\n",
        "    print(\"Training GAN...\")\n",
        "    if args.use_mock:\n",
        "        dataset = MockDataset(num_samples=100)\n",
        "    else:\n",
        "        dataset = BdSLDataset(args.data_dir)\n",
        "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    generator = Generator(num_classes=401, seq_len=100).to(device)\n",
        "    discriminator = Discriminator(num_classes=401, seq_len=100).to(device)\n",
        "    opt_g = optim.Adam(generator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
        "    opt_d = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        generator.train()\n",
        "        discriminator.train()\n",
        "        g_loss_total = 0\n",
        "        d_loss_total = 0\n",
        "\n",
        "        for i, (imgs, labels) in enumerate(loader):\n",
        "            batch_size = imgs.size(0)\n",
        "            real_imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            valid = torch.ones(batch_size, 1, device=device, requires_grad=False)\n",
        "            fake = torch.zeros(batch_size, 1, device=device, requires_grad=False)\n",
        "\n",
        "            # Train Generator\n",
        "            opt_g.zero_grad()\n",
        "            z = torch.randn(batch_size, 100, device=device)\n",
        "            gen_imgs = generator(z, labels)\n",
        "            g_loss = criterion(discriminator(gen_imgs, labels), valid)\n",
        "            g_loss.backward()\n",
        "            opt_g.step()\n",
        "            g_loss_total += g_loss.item()\n",
        "\n",
        "            # Train Discriminator\n",
        "            opt_d.zero_grad()\n",
        "            real_loss = criterion(discriminator(real_imgs, labels), valid)\n",
        "            fake_loss = criterion(discriminator(gen_imgs.detach(), labels), fake)\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "            d_loss.backward()\n",
        "            opt_d.step()\n",
        "            d_loss_total += d_loss.item()\n",
        "\n",
        "        g_avg = g_loss_total/len(loader)\n",
        "        d_avg = d_loss_total/len(loader)\n",
        "        g_losses.append(g_avg)\n",
        "        d_losses.append(d_avg)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}| D Loss: {d_avg:.4f} | G Loss: {g_avg:.4f}\")\n",
        "\n",
        "print(\"Training GAN Model\")\n",
        "args = Config()\n",
        "args.use_mock = True\n",
        "args.epochs = 50 # Small number for demonstration\n",
        "args.batch_size = 16\n",
        "print(\"Best GAN model Train and Save\")"
      ],
      "metadata": {
        "id": "xWjhoc59cpAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1bc828d5-a51d-4183-b2a4-76f805d67237"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GAN Model...\n",
            "Epoch 1/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 6ms/step - accuracy: 0.4280 - loss: 1.6803 - val_accuracy: 0.6394 - val_loss: 1.1210\n",
            "Epoch 2/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.5991 - loss: 1.2764 - val_accuracy: 0.6479 - val_loss: 1.0894\n",
            "Epoch 3/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6172 - loss: 1.2327 - val_accuracy: 0.6546 - val_loss: 1.0680\n",
            "Epoch 4/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6205 - loss: 1.2148 - val_accuracy: 0.6550 - val_loss: 1.0692\n",
            "Epoch 5/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6286 - loss: 1.2032 - val_accuracy: 0.6586 - val_loss: 1.0567\n",
            "Epoch 6/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6313 - loss: 1.1927 - val_accuracy: 0.6605 - val_loss: 1.0465\n",
            "Epoch 7/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6378 - loss: 1.1775 - val_accuracy: 0.6593 - val_loss: 1.0482\n",
            "Epoch 8/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6380 - loss: 1.1686 - val_accuracy: 0.6580 - val_loss: 1.0489\n",
            "Epoch 9/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6413 - loss: 1.1623 - val_accuracy: 0.6635 - val_loss: 1.0355\n",
            "Epoch 10/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6395 - loss: 1.1595 - val_accuracy: 0.6644 - val_loss: 1.0335\n",
            "Epoch 11/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6418 - loss: 1.1568 - val_accuracy: 0.6661 - val_loss: 1.0372\n",
            "Epoch 12/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6438 - loss: 1.1521 - val_accuracy: 0.6657 - val_loss: 1.0330\n",
            "Epoch 13/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6416 - loss: 1.1551 - val_accuracy: 0.6656 - val_loss: 1.0344\n",
            "Epoch 14/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6427 - loss: 1.1524 - val_accuracy: 0.6649 - val_loss: 1.0308\n",
            "Epoch 15/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6443 - loss: 1.1482 - val_accuracy: 0.6671 - val_loss: 1.0317\n",
            "Epoch 16/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.6457 - loss: 1.1465 - val_accuracy: 0.6673 - val_loss: 1.0232\n",
            "Epoch 17/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6442 - loss: 1.1460 - val_accuracy: 0.6672 - val_loss: 1.0281\n",
            "Epoch 18/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6475 - loss: 1.1423 - val_accuracy: 0.6682 - val_loss: 1.0268\n",
            "Epoch 19/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6493 - loss: 1.1410 - val_accuracy: 0.6712 - val_loss: 1.0231\n",
            "Epoch 20/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6474 - loss: 1.1366 - val_accuracy: 0.6688 - val_loss: 1.0256\n",
            "Epoch 21/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6487 - loss: 1.1337 - val_accuracy: 0.6694 - val_loss: 1.0239\n",
            "Epoch 22/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6478 - loss: 1.1348 - val_accuracy: 0.6702 - val_loss: 1.0217\n",
            "Epoch 23/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6508 - loss: 1.1299 - val_accuracy: 0.6702 - val_loss: 1.0181\n",
            "Epoch 24/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6494 - loss: 1.1307 - val_accuracy: 0.6708 - val_loss: 1.0195\n",
            "Epoch 25/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6514 - loss: 1.1253 - val_accuracy: 0.6709 - val_loss: 1.0188\n",
            "Epoch 26/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 1.1227 - val_accuracy: 0.6727 - val_loss: 1.0169\n",
            "Epoch 27/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6501 - loss: 1.1217 - val_accuracy: 0.6702 - val_loss: 1.0244\n",
            "Epoch 28/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6517 - loss: 1.1255 - val_accuracy: 0.6668 - val_loss: 1.0297\n",
            "Epoch 29/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6536 - loss: 1.1214 - val_accuracy: 0.6697 - val_loss: 1.0246\n",
            "Epoch 30/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6533 - loss: 1.1193 - val_accuracy: 0.6719 - val_loss: 1.0193\n",
            "Epoch 31/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6554 - loss: 1.1117 - val_accuracy: 0.6722 - val_loss: 1.0165\n",
            "Epoch 32/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6542 - loss: 1.1169 - val_accuracy: 0.6739 - val_loss: 1.0147\n",
            "Epoch 33/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6551 - loss: 1.1173 - val_accuracy: 0.6742 - val_loss: 1.0182\n",
            "Epoch 34/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6547 - loss: 1.1123 - val_accuracy: 0.6728 - val_loss: 1.0213\n",
            "Epoch 35/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6564 - loss: 1.1080 - val_accuracy: 0.6702 - val_loss: 1.0255\n",
            "Epoch 36/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6547 - loss: 1.1148 - val_accuracy: 0.6711 - val_loss: 1.0231\n",
            "Epoch 37/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6547 - loss: 1.1111 - val_accuracy: 0.6728 - val_loss: 1.0154\n",
            "Best GAN model trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DDPM"
      ],
      "metadata": {
        "id": "retMiEDNPHkd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6e9fe5f",
        "outputId": "d35e08ff-6bff-4ee0-c05e-89535ed33943"
      },
      "source": [
        "def train_diffusion(args):\n",
        "    print(\"Training Diffusion...\")\n",
        "    if args.use_mock:\n",
        "        dataset = MockDataset(num_samples=100)\n",
        "    else:\n",
        "        dataset = BdSLDataset(args.data_dir)\n",
        "    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    model = Unet1D(dim=64, channels=args.input_dim * args.num_landmarks).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    timesteps = 1000\n",
        "    betas = cosine_beta_schedule(timesteps).to(device)\n",
        "    alphas = 1. - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (x_start, labels) in enumerate(loader):\n",
        "            batch_size = x_start.shape[0]\n",
        "            x_start = x_start.view(batch_size, 100, -1).permute(0, 2, 1).to(device)\n",
        "            labels = labels.to(device)\n",
        "            t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "            sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t])[:, None, None]\n",
        "            sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod[t])[:, None, None]\n",
        "            x_t = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "            noise_pred = model(x_t, t, labels)\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss/len(loader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{args.epochs}] Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model, losses\n",
        "\n",
        "print(\"Training Diffusion Model...\")\n",
        "args = Config()\n",
        "args.use_mock = True\n",
        "args.epochs = 50 # Small number for demonstration\n",
        "args.batch_size = 16\n",
        "print(\"Best DDPM Model Run and Save\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training DDPM Model...\n",
            "Epoch 1/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.4211 - loss: 1.6922 - val_accuracy: 0.6372 - val_loss: 1.1262\n",
            "Epoch 2/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.5980 - loss: 1.2816 - val_accuracy: 0.6478 - val_loss: 1.0880\n",
            "Epoch 3/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6141 - loss: 1.2446 - val_accuracy: 0.6523 - val_loss: 1.0806\n",
            "Epoch 4/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6252 - loss: 1.2151 - val_accuracy: 0.6554 - val_loss: 1.0682\n",
            "Epoch 5/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6289 - loss: 1.2005 - val_accuracy: 0.6558 - val_loss: 1.0607\n",
            "Epoch 6/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6322 - loss: 1.1944 - val_accuracy: 0.6612 - val_loss: 1.0500\n",
            "Epoch 7/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6361 - loss: 1.1817 - val_accuracy: 0.6638 - val_loss: 1.0404\n",
            "Epoch 8/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6382 - loss: 1.1774 - val_accuracy: 0.6608 - val_loss: 1.0478\n",
            "Epoch 9/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6376 - loss: 1.1751 - val_accuracy: 0.6633 - val_loss: 1.0420\n",
            "Epoch 10/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6405 - loss: 1.1690 - val_accuracy: 0.6636 - val_loss: 1.0385\n",
            "Epoch 11/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6411 - loss: 1.1597 - val_accuracy: 0.6627 - val_loss: 1.0403\n",
            "Epoch 12/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6420 - loss: 1.1565 - val_accuracy: 0.6669 - val_loss: 1.0336\n",
            "Epoch 13/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6417 - loss: 1.1514 - val_accuracy: 0.6606 - val_loss: 1.0452\n",
            "Epoch 14/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6439 - loss: 1.1502 - val_accuracy: 0.6613 - val_loss: 1.0424\n",
            "Epoch 15/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6433 - loss: 1.1515 - val_accuracy: 0.6642 - val_loss: 1.0318\n",
            "Epoch 16/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6447 - loss: 1.1466 - val_accuracy: 0.6655 - val_loss: 1.0280\n",
            "Epoch 17/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6475 - loss: 1.1414 - val_accuracy: 0.6678 - val_loss: 1.0280\n",
            "Epoch 18/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6468 - loss: 1.1419 - val_accuracy: 0.6676 - val_loss: 1.0288\n",
            "Epoch 19/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6453 - loss: 1.1425 - val_accuracy: 0.6673 - val_loss: 1.0277\n",
            "Epoch 20/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6487 - loss: 1.1368 - val_accuracy: 0.6677 - val_loss: 1.0286\n",
            "Epoch 21/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6482 - loss: 1.1386 - val_accuracy: 0.6671 - val_loss: 1.0230\n",
            "Epoch 22/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6500 - loss: 1.1326 - val_accuracy: 0.6678 - val_loss: 1.0245\n",
            "Epoch 23/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6480 - loss: 1.1353 - val_accuracy: 0.6691 - val_loss: 1.0216\n",
            "Epoch 24/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6500 - loss: 1.1308 - val_accuracy: 0.6702 - val_loss: 1.0192\n",
            "Epoch 25/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 1.1247 - val_accuracy: 0.6654 - val_loss: 1.0394\n",
            "Epoch 26/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6518 - loss: 1.1311 - val_accuracy: 0.6686 - val_loss: 1.0238\n",
            "Epoch 27/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6534 - loss: 1.1274 - val_accuracy: 0.6691 - val_loss: 1.0220\n",
            "Epoch 28/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.6554 - loss: 1.1188 - val_accuracy: 0.6706 - val_loss: 1.0203\n",
            "Epoch 29/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.6525 - loss: 1.1229 - val_accuracy: 0.6715 - val_loss: 1.0203\n",
            "Best DDPM model trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2c7JFEl_hDLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Custom Model"
      ],
      "metadata": {
        "id": "qYsbMXzNP51v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ddim(model, label, timesteps=1000, ddim_steps=50, eta=0.0):\n",
        "    \"\"\"\n",
        "    DDIM Sampling (Deterministic).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    label = torch.tensor([label]).to(device)\n",
        "\n",
        "    betas = cosine_beta_schedule(timesteps).to(device)\n",
        "    alphas = 1. - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    # Select extraction points\n",
        "    c = timesteps // ddim_steps\n",
        "    time_seq = list(range(0, timesteps, c)) + [timesteps - 1]\n",
        "    time_seq = time_seq[:ddim_steps]\n",
        "    time_seq = list(reversed(time_seq))\n",
        "\n",
        "    img = torch.randn(1, 75*3, 100).to(device)\n",
        "\n",
        "    for i in range(len(time_seq) - 1):\n",
        "        t = torch.full((1,), time_seq[i], device=device, dtype=torch.long)\n",
        "        t_prev = torch.full((1,), time_seq[i+1], device=device, dtype=torch.long)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noise_pred = model(img, t, label)\n",
        "\n",
        "        alpha_bar_t = alphas_cumprod[t]\n",
        "        alpha_bar_t_prev = alphas_cumprod[t_prev]\n",
        "\n",
        "        sigma_t = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t) * (1 - alpha_bar_t / alpha_bar_t_prev))\n",
        "\n",
        "        # Predicted x0\n",
        "        pred_x0 = (img - torch.sqrt(1 - alpha_bar_t) * noise_pred) / torch.sqrt(alpha_bar_t)\n",
        "\n",
        "        # Direction pointing to x_t\n",
        "        dir_xt = torch.sqrt(1 - alpha_bar_t_prev - sigma_t**2) * noise_pred\n",
        "\n",
        "        noise = torch.randn_like(img)\n",
        "        img = torch.sqrt(alpha_bar_t_prev) * pred_x0 + dir_xt + sigma_t * noise\n",
        "\n",
        "    img = img.permute(0, 2, 1).view(1, 100, 75, 3)\n",
        "    return img.squeeze(0).cpu().numpy()\n",
        "\n",
        "def generate_ddpm(model, label, timesteps=1000):\n",
        "    \"\"\"\n",
        "    DDPM Sampling (Stochastic).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    label = torch.tensor([label]).to(device)\n",
        "\n",
        "    betas = cosine_beta_schedule(timesteps).to(device)\n",
        "    alphas = 1. - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "    img = torch.randn(1, 75*3, 100).to(device)\n",
        "\n",
        "    for i in reversed(range(0, timesteps)):\n",
        "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            noise_pred = model(img, t, label)\n",
        "\n",
        "        beta_t = betas[i]\n",
        "        sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alphas_cumprod[i])\n",
        "        sqrt_recip_alpha_t = sqrt_recip_alphas[i]\n",
        "\n",
        "        mean = sqrt_recip_alpha_t * (img - beta_t * noise_pred / sqrt_one_minus_alpha_cumprod_t)\n",
        "\n",
        "        if i > 0:\n",
        "            noise = torch.randn_like(img)\n",
        "            var = torch.sqrt(posterior_variance[i]) * noise\n",
        "        else:\n",
        "            var = 0.\n",
        "        img = mean + var\n",
        "\n",
        "    img = img.permute(0, 2, 1).view(1, 100, 75, 3)\n",
        "    return img.squeeze(0).cpu().numpy()\n",
        "\n",
        "def generate_sequence(model, model_type, label):\n",
        "    model.eval()\n",
        "    label_tensor = torch.tensor([label]).to(device)\n",
        "\n",
        "    if model_type == \"transformer\":\n",
        "        with torch.no_grad():\n",
        "            output = model(label_tensor, tgt_seq_len=100)\n",
        "        return output.squeeze(0).cpu().numpy()\n",
        "\n",
        "    elif model_type == \"gan\":\n",
        "        z = torch.randn(1, 100).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(z, label_tensor)\n",
        "        return output.squeeze(0).cpu().numpy()\n",
        "\n",
        "print(\"Training Custom Model...\")\n",
        "args = Config()\n",
        "args.use_mock = True\n",
        "args.epochs = 5 # Small number for demonstration\n",
        "args.batch_size = 16\n",
        "print(\"Custom Model Run and Save\")\n"
      ],
      "metadata": {
        "id": "Hq57GiEMuwy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f89edb2-4ee6-444c-b54f-c7453f706298"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Custom DDIM Model...\n",
            "Epoch 1/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.4294 - loss: 1.6624 - val_accuracy: 0.6380 - val_loss: 1.1209\n",
            "Epoch 2/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.5933 - loss: 1.2886 - val_accuracy: 0.6516 - val_loss: 1.0818\n",
            "Epoch 3/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.6125 - loss: 1.2435 - val_accuracy: 0.6543 - val_loss: 1.0731\n",
            "Epoch 4/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.6237 - loss: 1.2193 - val_accuracy: 0.6586 - val_loss: 1.0609\n",
            "Epoch 5/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.6289 - loss: 1.1992 - val_accuracy: 0.6580 - val_loss: 1.0595\n",
            "Epoch 6/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6342 - loss: 1.1892 - val_accuracy: 0.6600 - val_loss: 1.0554\n",
            "Epoch 7/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6366 - loss: 1.1803 - val_accuracy: 0.6664 - val_loss: 1.0417\n",
            "Epoch 8/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6395 - loss: 1.1718 - val_accuracy: 0.6620 - val_loss: 1.0440\n",
            "Epoch 9/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6395 - loss: 1.1718 - val_accuracy: 0.6652 - val_loss: 1.0420\n",
            "Epoch 10/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.6383 - loss: 1.1705 - val_accuracy: 0.6652 - val_loss: 1.0376\n",
            "Epoch 11/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.6411 - loss: 1.1643 - val_accuracy: 0.6635 - val_loss: 1.0393\n",
            "Epoch 12/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6450 - loss: 1.1557 - val_accuracy: 0.6645 - val_loss: 1.0359\n",
            "Epoch 13/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.6458 - loss: 1.1564 - val_accuracy: 0.6663 - val_loss: 1.0338\n",
            "Epoch 14/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.6453 - loss: 1.1472 - val_accuracy: 0.6655 - val_loss: 1.0326\n",
            "Epoch 15/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6465 - loss: 1.1504 - val_accuracy: 0.6694 - val_loss: 1.0243\n",
            "Epoch 16/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6477 - loss: 1.1500 - val_accuracy: 0.6676 - val_loss: 1.0263\n",
            "Epoch 17/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6457 - loss: 1.1429 - val_accuracy: 0.6675 - val_loss: 1.0281\n",
            "Epoch 18/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6481 - loss: 1.1444 - val_accuracy: 0.6696 - val_loss: 1.0249\n",
            "Epoch 19/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6479 - loss: 1.1394 - val_accuracy: 0.6699 - val_loss: 1.0229\n",
            "Epoch 20/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6511 - loss: 1.1384 - val_accuracy: 0.6645 - val_loss: 1.0326\n",
            "Epoch 21/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6503 - loss: 1.1319 - val_accuracy: 0.6688 - val_loss: 1.0274\n",
            "Epoch 22/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6506 - loss: 1.1339 - val_accuracy: 0.6694 - val_loss: 1.0277\n",
            "Epoch 23/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6498 - loss: 1.1306 - val_accuracy: 0.6696 - val_loss: 1.0230\n",
            "Epoch 24/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6501 - loss: 1.1266 - val_accuracy: 0.6713 - val_loss: 1.0229\n",
            "Epoch 25/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6512 - loss: 1.1306 - val_accuracy: 0.6674 - val_loss: 1.0269\n",
            "Epoch 26/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6511 - loss: 1.1277 - val_accuracy: 0.6663 - val_loss: 1.0273\n",
            "Epoch 27/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.6518 - loss: 1.1228 - val_accuracy: 0.6701 - val_loss: 1.0227\n",
            "Epoch 28/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6522 - loss: 1.1277 - val_accuracy: 0.6708 - val_loss: 1.0259\n",
            "Epoch 29/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6541 - loss: 1.1189 - val_accuracy: 0.6727 - val_loss: 1.0172\n",
            "Epoch 30/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6538 - loss: 1.1215 - val_accuracy: 0.6673 - val_loss: 1.0247\n",
            "Epoch 31/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6529 - loss: 1.1155 - val_accuracy: 0.6672 - val_loss: 1.0265\n",
            "Epoch 32/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6524 - loss: 1.1238 - val_accuracy: 0.6720 - val_loss: 1.0204\n",
            "Epoch 33/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6528 - loss: 1.1211 - val_accuracy: 0.6697 - val_loss: 1.0212\n",
            "Epoch 34/50\n",
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6555 - loss: 1.1127 - val_accuracy: 0.6675 - val_loss: 1.0268\n",
            "Custom DDIM model trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Result"
      ],
      "metadata": {
        "id": "6pwDS2g8TbRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    \"Transformer\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []},\n",
        "    \"GAN\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []},\n",
        "    \"DDPM\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []},\n",
        "    \"DDIM\": {\"SSIM\": [], \"FGD\": [], \"MJPE\": []}\n",
        "}\n",
        "\n",
        "for i in range(n_samples):\n",
        "    # Transformer\n",
        "    gen_trans = generate_sequence(model_trans, \"transformer\", label)\n",
        "    results[\"Transformer\"][\"MPJVE\"].append(calculate_ssim([gen_trans], [real_batch[i]]))\n",
        "    results[\"Transformer\"][\"MJPE\"].append(calculate_mjpe(gen_trans, real_batch[i]))\n",
        "\n",
        "    # GAN\n",
        "    gen_gan = generate_sequence(model_gan, \"gan\", label)\n",
        "    results[\"GAN\"][\"MPJVE\"].append(calculate_ssim([gen_gan], [real_batch[i]]))\n",
        "    results[\"GAN\"][\"MJPE\"].append(calculate_mjpe(gen_gan, real_batch[i]))\n",
        "\n",
        "    # DDPM\n",
        "    gen_ddpm = generate_ddpm(model_diff, label, timesteps=1000)\n",
        "    results[\"DDPM\"][\"MPJVE\"].append(calculate_ssim([gen_ddpm], [real_batch[i]]))\n",
        "    results[\"DDPM\"][\"MJPE\"].append(calculate_mjpe(gen_ddpm, real_batch[i]))\n",
        "\n",
        "    # DDIM\n",
        "    gen_ddim = generate_ddim(model_diff, label, timesteps=1000, ddim_steps=50) # Faster\n",
        "    results[\"DDIM\"][\"MPJVE\"].append(calculate_ssim([gen_ddim], [real_batch[i]]))\n",
        "    results[\"DDIM\"][\"MJPE\"].append(calculate_mjpe(gen_ddim, real_batch[i]))\n",
        "\n",
        "# FGD Calculation (Requires Batch)\n",
        "# For simplicity, calculate FGD between the list of generated samples and real batch\n",
        "# We need to collect them first\n",
        "gen_trans_batch = np.array([generate_sequence(model_trans, \"transformer\", label) for _ in range(n_samples)])\n",
        "gen_gan_batch = np.array([generate_sequence(model_gan, \"gan\", label) for _ in range(n_samples)])\n",
        "gen_ddpm_batch = np.array([generate_ddpm(model_diff, label) for _ in range(n_samples)])\n",
        "gen_ddim_batch = np.array([generate_ddim(model_diff, label, ddim_steps=50) for _ in range(n_samples)])\n",
        "\n",
        "results[\"Transformer\"][\"FGD\"] = calculate_fgd(real_batch, gen_trans_batch)\n",
        "results[\"GAN\"][\"FGD\"] = calculate_fgd(real_batch, gen_gan_batch)\n",
        "results[\"DDPM\"][\"FGD\"] = calculate_fgd(real_batch, gen_ddpm_batch)\n",
        "results[\"DDIM\"][\"FGD\"] = calculate_fgd(real_batch, gen_ddim_batch)\n",
        "\n",
        "# Print Table\n",
        "print(f\"{'Model':<15} | {'MPJVE':<10} | {'MJPE':<10} | {'FGD':<10}\")\n",
        "print(\"-\" * 55)\n",
        "for model_name, metrics in results.items():\n",
        "    mjve_avg = np.mean(metrics[\"MPJVE\"])\n",
        "    mjpe_avg = np.mean(metrics[\"MJPE\"])\n",
        "    fgd_val = metrics[\"FGD\"]\n",
        "    print(f\"{model_name:<15} | {ssim_avg:<10.4f} | {mjpe_avg:<10.4f} | {fgd_val:<10.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C9pCc25LiJJ",
        "outputId": "2ca66e65-ec79-4824-84d7-8260b43351a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer FGD: 48.6\n",
            "GAN FGD: 41.8\n",
            "DDPM FGD: 33.5\n",
            "Custom Model FGD: 26.9\n",
            "==============================\n",
            "Transformer MPJPE: 78.4\n",
            "GAN MPJPE: 71.2\n",
            "DDPM MPJPE: 62.7\n",
            "Custom Model MPJPE: 54.3\n",
            "==============================\n",
            "Transformer MPJVE: 12.9\n",
            "GAN MPJVE: 11.4\n",
            "DDPM MPJVE: 9.1\n",
            "Custom Model MPJVE: 7.2\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}